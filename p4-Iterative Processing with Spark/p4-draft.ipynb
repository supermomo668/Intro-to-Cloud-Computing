{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fbf16bff-a9f9-42e3-b327-a5e29ae030b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://host.docker.internal:4040\n",
       "SparkContext available as 'sc' (version = 3.2.1, master = local[*], app id = local-1651658034730)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.SparkSession\n",
       "import org.apache.spark.util.Utils\n",
       "import org.apache.spark.sql.types._\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.SparkSession\n",
    "import org.apache.spark.util.Utils\n",
    "import org.apache.spark.sql.types._\n",
    "//import org.apache.spark.util.AccumulatorV2\n",
    "\n",
    "// val spark = SparkSession\n",
    "//    .builder()\n",
    "//    .master(\"local[*]\")\n",
    "//    .appName(\"SparkSessionExample\")\n",
    "//    .getOrCreate()\n",
    "// val sc = spark.sparkContext\n",
    "\n",
    "// val graphRDD = sc.textFile(\"wasb://datasets@clouddeveloper.blob.core.windows.net/iterative-processing/Graph\")\n",
    "// val graphRDD = sc.textFile(\"./SmallGraph1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "343a2d51-8e0b-4b4b-84ea-06c0f018e986",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.SparkConf\n",
       "import org.apache.spark.sql.SparkSession\n",
       "defined object SparkUtils\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.SparkConf\n",
    "import org.apache.spark.sql.SparkSession\n",
    "object SparkUtils {\n",
    "  def sparkSession(): SparkSession = {\n",
    "    val conf = new SparkConf().setAppName(\"PageRank\")\n",
    "    SparkSession.builder.master(\"local[*]\").config(conf).getOrCreate()\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbb96b6a-4abb-4eaf-8b3f-c60593724662",
   "metadata": {},
   "outputs": [],
   "source": [
    "// Load data\n",
    "//val spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "var schema = StructType(Array(\n",
    "    StructField(\"follower\",   StringType, true),\n",
    "    StructField(\"followee\",   StringType, true))\n",
    ")   \n",
    "val df = spark.read.format(\"csv\")\n",
    "    .option(\"sep\",\"\\t\")\n",
    "    //.option(\"header\", \"true\")\n",
    "    .schema(schema)\n",
    "    .load(\"./SmallGraph1\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9945171a-a1cf-47cb-a92f-7a0b6fdc9d8c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "edges: Long = 6\n",
       "follower_df: org.apache.spark.sql.DataFrame = [follower: string]\n",
       "followee_df: org.apache.spark.sql.DataFrame = [followee: string]\n",
       "vertices: Long = 4\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// count distinct edges\n",
    "val edges = df.distinct().count()\n",
    "\n",
    "// count distinct vertices\n",
    "val follower_df = df.select(\"follower\")\n",
    "val followee_df = df.select(\"followee\")\n",
    "\n",
    "val vertices = follower_df.union(followee_df).distinct().count()\n",
    "//count.collect().foreach(println)\n",
    "println(followee_df.collect()(0)(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ff301aa6-450a-4df7-879d-51e8c565c5a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import java.nio.charset.StandardCharsets\n",
       "import java.nio.file.{Files, Paths}\n",
       "import org.apache.spark.SparkContext\n",
       "import org.apache.spark.sql.SparkSession\n",
       "import org.apache.spark.sql.types._\n",
       "defined object VerticesEdgesCounts\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import java.nio.charset.StandardCharsets\n",
    "import java.nio.file.{Files, Paths}\n",
    "\n",
    "import org.apache.spark.SparkContext\n",
    "import org.apache.spark.sql.SparkSession\n",
    "import org.apache.spark.sql.types._\n",
    "\n",
    "object VerticesEdgesCounts {\n",
    "\n",
    "  /**\n",
    "    */\n",
    "  def verticesAndNodesCount(inputPath: String, spark: SparkSession, sc: SparkContext): String = {\n",
    "    // TODO: copy your code from the notebook here.\n",
    "    val spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "    var schema = StructType(Array(\n",
    "        StructField(\"follower\",   IntegerType, true),\n",
    "        StructField(\"followee\",   IntegerType, true))\n",
    "    )   \n",
    "    val df = spark.read.format(\"csv\")\n",
    "        .option(\"sep\",\"\\t\")\n",
    "        .schema(schema)\n",
    "        .load(inputPath)\n",
    "    // TODO: replace with the actual values. You should not hardcode them as the grader tests the\n",
    "        // count distinct vertices\n",
    "    val follower_id = df.select(\"follower\")\n",
    "    val followee_id = df.select(\"followee\")    \n",
    "    \n",
    "    //  function on a secret dataset.\n",
    "    //val edges = 0L\n",
    "    // count distinct edges\n",
    "    val edges = df.count()\n",
    "    // union ids\n",
    "    //val vertices = 0L\n",
    "    val vertices = follower_id.union(followee_id).distinct.count() // find distinct\n",
    "    println(vertices)\n",
    "    println(edges)\n",
    "    countsToString(vertices, edges)\n",
    "  }\n",
    "\n",
    "  /**\n",
    "   * @param args it should be called with two arguments, the input path, and the output path.\n",
    "   */\n",
    "  def main(args: Array[String]): Unit = {\n",
    "    val spark = SparkUtils.sparkSession()\n",
    "    val sc = spark.sparkContext\n",
    "\n",
    "    val inputPath = args(0)\n",
    "    val outputPath = args(1)\n",
    "\n",
    "    val outputString = verticesAndNodesCount(inputPath, spark, sc)\n",
    "    saveToFile(outputString, outputPath)\n",
    "  }\n",
    "\n",
    "  /**\n",
    "    * Formats the vertices and edges counts in the format expected by the grader.\n",
    "    * @param numVertices the number of vertices.\n",
    "    * @param numEdges the number of edges.\n",
    "    * @return a string with the vertices and edges counts in the format expected by the grader.\n",
    "    */\n",
    "  def countsToString(numVertices: Long, numEdges: Long): String =\n",
    "    s\"\"\"|num_vertices=$numVertices\n",
    "        |num_edges=$numEdges\"\"\".stripMargin\n",
    "\n",
    "  /**\n",
    "   * Saves the output string to the output path.\n",
    "   * @param outputString the string to save.\n",
    "   * @param outputPath the file to save to.\n",
    "   */\n",
    "  def saveToFile(outputString: String, outputPath: String): Unit =\n",
    "    Files.write(Paths.get(outputPath), outputString.getBytes(StandardCharsets.UTF_8))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e38e69b8-b6db-47a1-8d3e-a41f89393b1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "6\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "input_path: String = ./SmallGraph1\n",
       "output_path: String = ./answer_task1\n"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val input_path = \"./SmallGraph1\"\n",
    "val output_path = \"./answer_task1a\"\n",
    "\n",
    "VerticesEdgesCounts.main(Array(input_path, output_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f761852-9142-4d71-85d7-50d3fcb9d2e4",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Part B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "75e0ca45-8951-4318-8f52-5fcaa3b7af96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.SparkSession\n",
       "defined object FollowerDF\n"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Follower DF\n",
    "import org.apache.spark.sql.SparkSession\n",
    "\n",
    "object FollowerDF {\n",
    "    def computeFollowerCountDF(inputPath: String, outputPath: String, spark: SparkSession): Unit = {\n",
    "        // TODO: Calculate the follower count for each user\n",
    "          // get dataset\n",
    "        var schema = StructType(Array(\n",
    "            StructField(\"follower\",   StringType, true),\n",
    "            StructField(\"followee\",   StringType, true))\n",
    "        )   \n",
    "        val df = spark.read.format(\"csv\")\n",
    "            .option(\"sep\",\"\\t\")\n",
    "            //.option(\"header\", \"true\")\n",
    "            .schema(schema)\n",
    "            .load(inputPath)\n",
    "\n",
    "        // TODO: Write the top 100 users to the above outputPath in parquet format\n",
    "        val count_df = df.groupBy(\"followee\")\n",
    "        .agg(\n",
    "          countDistinct(\"follower\").as(\"count\")\n",
    "        ).sort(desc(\"count\"))\n",
    "        count_df.show(100)\n",
    "        count_df.write.parquet(outputPath)\n",
    "    }\n",
    "\n",
    "  /**\n",
    "    * @param args it should be called with two arguments, the input path, and the output path.\n",
    "    */\n",
    "    def main(args: Array[String]): Unit = {\n",
    "        val spark = SparkUtils.sparkSession()\n",
    "        val inputGraph = args(0)\n",
    "        val followerDFOutputPath = args(1)\n",
    "\n",
    "        computeFollowerCountDF(inputGraph, followerDFOutputPath, spark)\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7eded93b-131d-4beb-8031-a7fe60d4e70b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+\n",
      "|followee|count|\n",
      "+--------+-----+\n",
      "|       3|    2|\n",
      "|       2|    2|\n",
      "|       0|    1|\n",
      "|       1|    1|\n",
      "+--------+-----+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "input_path: String = ./SmallGraph1\n",
       "output_path: String = ./answer_ptb-task1\n"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "val input_path = \"./SmallGraph1\"\n",
    "val output_path = \"./answer_task1-ptb1\"\n",
    "\n",
    "FollowerDF.main(Array(input_path, output_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b400137e-935b-4200-9ee5-d9bf2a705e4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined object FollowerRDD\n"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// FollowerRDD\n",
    "object FollowerRDD {\n",
    "\n",
    "    def computeFollowerCountRDD(\n",
    "      inputPath: String,\n",
    "      outputPath: String,\n",
    "      sc: SparkContext): Unit = {\n",
    "        // TODO: Calculate the follower count for each user\n",
    "        //  read file with rdd\n",
    "        val file_rdd = sc.textFile(input_path)\n",
    "        val graph = file_rdd\n",
    "          .map(row => row.split(\"\\t\"))\n",
    "          .map(x => (x(1), x(0))) // map followee as key\n",
    "        // TODO: Write the top 100 users to the outputPath with userID and count **tab separated**\n",
    "        val popular_followee = graph\n",
    "          //.distinct() // remove duplicate\n",
    "          .map(x => (x._1, 1)) // count\n",
    "          .reduceByKey(_ + _) // sum counts\n",
    "          .sortBy(x => x._2, ascending=false) // sort by number of followers\n",
    "          .take(100)      // .keys.take(100)\n",
    "          // take top 100\n",
    "\n",
    "        val result = sc.parallelize(popular_followee).map(f => f._1 +\"\\t\"+f._2)\n",
    "        println(result.take(5).mkString(\"\\t\"))\n",
    "        // popular_followee.map(r => (r._1+\"\\t\"r._2.toString)).saveAsTextFile(\"outputCSV2\")\n",
    "        result.saveAsTextFile(outputPath)\n",
    "      }\n",
    "\n",
    "  /**\n",
    "    * @param args it should be called with two arguments, the input path, and the output path.\n",
    "    */\n",
    "  def main(args: Array[String]): Unit = {\n",
    "    val spark = SparkUtils.sparkSession()\n",
    "    val sc = spark.sparkContext\n",
    "\n",
    "    val inputGraph = args(0)\n",
    "    val followerRDDOutputPath = args(1)\n",
    "\n",
    "    computeFollowerCountRDD(inputGraph, followerRDDOutputPath, sc)\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "09bbff03-6885-4863-825f-b19e79e3d5ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\t2\t3\t2\t0\t1\t1\t1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "input_path: String = ./SmallGraph1\n",
       "output_path: String = ./answer_ptb-task2\n"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "val input_path = \"./SmallGraph1\"\n",
    "val output_path = \"./answer_task1-ptb2\"\n",
    "\n",
    "FollowerRDD.main(Array(input_path, output_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "068e8b1b-e5c8-4476-8b31-1bc43e91f287",
   "metadata": {},
   "source": [
    "# Part 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee92dc7c-2b6f-46a1-b10e-cc8f8a7babb3",
   "metadata": {},
   "source": [
    "My Full working: with debugging printout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "id": "52ae903f-116b-49d1-937c-b0d4f03d8409",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined object PageRank\n"
      ]
     },
     "execution_count": 318,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "object PageRank {\n",
    "\n",
    "    // Do not modify\n",
    "    val PageRankIterations = 10\n",
    "\n",
    "    def calculatePageRank(\n",
    "        inputGraphPath: String,\n",
    "        outputPath: String,\n",
    "        iterations: Int,\n",
    "        spark: SparkSession): Unit = {\n",
    "        val sc = spark.sparkContext\n",
    "        \n",
    "        // TODO - Your code here\n",
    "        // params\n",
    "        val damping = 0.85 \n",
    "        // map followee as key \n",
    "        val edgelist = sc.textFile(inputGraphPath)\n",
    "        val followers_list = edgelist.map{   // id(0)   // or use row(1)\n",
    "          _.split(\"\\\\s+\")(0)\n",
    "        }\n",
    "        // val n_followers = followers.count()\n",
    "        // obtain all followee \n",
    "        val followees_list = edgelist.map{\n",
    "          _.split(\"\\\\s+\")(1)\n",
    "        }.distinct()\n",
    "        //val n_followees = followees_list.count()\n",
    "        // all body\n",
    "        val all_ids = followers_list.union(followees_list).distinct()\n",
    "        //val n_users = 4    \n",
    "        val n_users = all_ids.count()   // already have        \n",
    "        println(\"Number of users & IDs:\")\n",
    "        println(n_users)\n",
    "        println(all_ids.collect().mkString(\" \"))\n",
    "        // followers_only\n",
    "        val followers_only = followers_list.subtract(followees_list).distinct()\n",
    "        println(\"Num Followers only:\")\n",
    "        println(followers_only.collect().mkString(\" \"))\n",
    "        \n",
    "        // dangnling ids\n",
    "        var dangling_edges = all_ids.subtract(followers_list).map(id => (id, \"e\")).distinct()\n",
    "        val n_dangling = dangling_edges.count()\n",
    "        println(\"Danglers\")\n",
    "        println(n_dangling)\n",
    "        println(dangling_edges.collect().mkString(\" \"))\n",
    "        // Form adj graph\n",
    "        // graph adj_list (no dangling)\n",
    "        val glinks = edgelist.map{ row => { // graph edges\n",
    "          val edge = row.split(\"\\\\s+\") \n",
    "            (edge(0), edge(1))\n",
    "        }}\n",
    "        ////////////////////////////////////////////////////\n",
    "        // Initialization\n",
    "              // full adjacent list\n",
    "        val glinks_all = glinks.union(dangling_edges).groupByKey().cache()  \n",
    "          // ranks \n",
    "        val default_rank = 1.0/n_users  // or with just 1.0\n",
    "        var ranks = glinks_all.mapValues{ v => default_rank}\n",
    "        var followers_only_rank = followers_only.map(id => (id, 0.0)).cache()\n",
    "        var is_dangling: Boolean = false\n",
    "        println(\"START Links\")\n",
    "        println(glinks_all.collect().mkString(\" \"))\n",
    "        println(\"START Rank:\")\n",
    "        println(ranks.collect().mkString(\" \"))\n",
    "        if (iterations > 0) {\n",
    "            // only allow if iterations is not 0\n",
    "            for (i <- 1 to iterations) {\n",
    "                println(\"Iteration:\")\n",
    "                println(i)\n",
    "                var dangling_ranks = sc.doubleAccumulator(\"sumAccumulator\")\n",
    "                val ranking = glinks_all.join(ranks).values   // (CompactBuffer(2),0.25) (CompactBuffer(0, 2, 3),0.25) (CompactBuffer(3),0.25) (CompactBuffer(e),0.25)\n",
    "                print(\"Iterator\")\n",
    "                ranking.foreach(tupl => println(tupl))\n",
    "                val scores = ranking.flatMap { case (followee, rank) =>  // contrib (url, rank)\n",
    "                    // \n",
    "                    var is_dangling: Boolean = followee.count {v => v == \"e\" } == 1\n",
    "                    if (! is_dangling || followee.size != 1) { \n",
    "                        // this is not a dangling edge\n",
    "                        followee.map(id => {  // 0, 2, 3... 2, 3... e\n",
    "                            (id, rank / followee.size)   // number of followee\n",
    "                        })\n",
    "                    } else {\n",
    "                        //summing up the dangling ranks\n",
    "                        followee.map(id => {\n",
    "                            dangling_ranks.add(rank)\n",
    "                        })\n",
    "                        List()   // set as list\n",
    "                        //dangling_ranks.add(rank)  instead of +=                        \n",
    "                    }\n",
    "                }\n",
    "                // scores.count() // no need if printing\n",
    "                println(\"Scores:\")\n",
    "                println(scores.foreach(tupl =>println(tupl)))\n",
    "                /// compute updated rank scores here\n",
    "                // use to calculate score\n",
    "                    //scores.count()//\n",
    "                val dangling_sum = dangling_ranks.value\n",
    "                ranks = scores.union(followers_only_rank)\n",
    "                    .reduceByKey(_+_).mapValues(\n",
    "                    scores =>\n",
    "                    // (1-damping) * dangling_ranks.value / n_users + damping * scores  // vert algo formula\n",
    "                    (1 - damping)/n_users + damping * (scores +  dangling_sum / n_users)\n",
    "                    //0.15 + 0.85 * (scores + dangling_ranks.value / n_users)\n",
    "                ).cache()  // (0,0.25) (1,0.25) (2,0.25) (3,0.25)\n",
    "                // ranks = ranks.union(followers_only.map(id => (id,  (1-damping)*n_users + damping*rank_remain/n_users)))\n",
    "                println(\"ranks after:\")\n",
    "                println(ranks.collect().mkString(\" \"))\n",
    "                // ranks = ranks.union(followers_only.map(id => (id, (1 - damping)/n_users +  damping * dangling_sum/ n_users)))\n",
    "            }\n",
    "        }  \n",
    "        // normalize\n",
    "        val rank_sum = ranks.map(e => e._2).sum()\n",
    "        // write to file\n",
    "        //ranks.map{case(vert, ranks) => s\"$vert\\t$ranks\"}.saveAsTextFile(outputPath)\n",
    "        // ranks.map{tup => s\"${tup._1}\\t${tup._2}\"}.saveAsTextFile(outputPath)\n",
    "        println(\"Final result\")\n",
    "        ranks.mapValues( s => s/rank_sum).foreach(tup => println(s\"${tup._1} has rank:  ${tup._2} .\"))\n",
    "    }\n",
    "    /**\n",
    "    * @param args it should be called with two arguments, the input path, and the output path.\n",
    "    */\n",
    "    def main(args: Array[String]): Unit = {\n",
    "        val spark = SparkUtils.sparkSession()\n",
    "\n",
    "        val inputGraph = args(0)\n",
    "        val pageRankOutputPath = args(1)\n",
    "        val PageRankIterations = 10\n",
    "        calculatePageRank(inputGraph, pageRankOutputPath, PageRankIterations, spark)\n",
    "  }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "id": "9b89b834-3194-4e62-99e5-85775bf8c33b",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of users & IDs:\n",
      "4\n",
      "0 1 2 3\n",
      "Num Followers only:\n",
      "1\n",
      "Danglers\n",
      "1\n",
      "(3,e)\n",
      "START Links\n",
      "(0,CompactBuffer(2)) (1,CompactBuffer(0, 2, 3)) (2,CompactBuffer(3)) (3,CompactBuffer(e))\n",
      "START Rank:\n",
      "(0,0.25) (1,0.25) (2,0.25) (3,0.25)\n",
      "Iteration:\n",
      "1\n",
      "Iterator(CompactBuffer(e),0.25)\n",
      "(CompactBuffer(2),0.25)\n",
      "(CompactBuffer(3),0.25)\n",
      "(CompactBuffer(0, 2, 3),0.25)\n",
      "Scores:\n",
      "(3,0.25)\n",
      "(2,0.25)\n",
      "(0,0.08333333333333333)\n",
      "(2,0.08333333333333333)\n",
      "(3,0.08333333333333333)\n",
      "()\n",
      "ranks after:\n",
      "(0,0.16145833333333331) (1,0.09062500000000001) (2,0.3739583333333333) (3,0.3739583333333333)\n",
      "Iteration:\n",
      "2\n",
      "Iterator(CompactBuffer(2),0.16145833333333331)\n",
      "(CompactBuffer(0, 2, 3),0.09062500000000001)\n",
      "(CompactBuffer(3),0.3739583333333333)\n",
      "(CompactBuffer(e),0.3739583333333333)\n",
      "Scores:\n",
      "(3,0.3739583333333333)\n",
      "(0,0.030208333333333337)\n",
      "(2,0.030208333333333337)\n",
      "(3,0.030208333333333337)\n",
      "(2,0.16145833333333331)\n",
      "()\n",
      "ranks after:\n",
      "(0,0.14264322916666666) (1,0.11696614583333333) (2,0.2798828125) (3,0.46050781249999995)\n",
      "Iteration:\n",
      "3\n",
      "Iterator(CompactBuffer(2),0.14264322916666666)\n",
      "(CompactBuffer(3),0.2798828125)\n",
      "(CompactBuffer(0, 2, 3),0.11696614583333333)\n",
      "(CompactBuffer(e),0.46050781249999995)\n",
      "Scores:\n",
      "(0,0.03898871527777777)\n",
      "(2,0.03898871527777777)\n",
      "(3,0.03898871527777777)\n",
      "(3,0.2798828125)\n",
      "(2,0.14264322916666666)\n",
      "()\n",
      "ranks after:\n",
      "(0,0.1684983181423611) (1,0.13535791015624998) (2,0.2897450629340277) (3,0.40639870876736106)\n",
      "Iteration:\n",
      "4\n",
      "Iterator(CompactBuffer(2),0.1684983181423611)\n",
      "(CompactBuffer(0, 2, 3),0.13535791015624998)\n",
      "(CompactBuffer(e),0.40639870876736106)\n",
      "(CompactBuffer(3),0.2897450629340277)\n",
      "Scores:\n",
      "(3,0.2897450629340277)\n",
      "(2,0.1684983181423611)\n",
      "(0,0.04511930338541666)\n",
      "(2,0.04511930338541666)\n",
      "(3,0.04511930338541666)\n",
      "()\n",
      "ranks after:\n",
      "(0,0.1622111334906684) (1,0.12385972561306423) (2,0.30543470391167526) (3,0.40849443698459187)\n",
      "Iteration:\n",
      "5\n",
      "Iterator(CompactBuffer(0, 2, 3),0.12385972561306423)\n",
      "(CompactBuffer(e),0.40849443698459187)\n",
      "(CompactBuffer(2),0.1622111334906684)\n",
      "(CompactBuffer(3),0.30543470391167526)\n",
      "Scores:\n",
      "(2,0.1622111334906684)\n",
      "(0,0.04128657520435474)\n",
      "(2,0.04128657520435474)\n",
      "(3,0.04128657520435474)\n",
      "(3,0.30543470391167526)\n",
      "()\n",
      "ranks after:\n",
      "(0,0.1593986567829273) (1,0.12430506785922578) (2,0.29727812024999545) (3,0.41901815510785123)\n",
      "Iteration:\n",
      "6\n",
      "Iterator(CompactBuffer(0, 2, 3),0.12430506785922578)\n",
      "(CompactBuffer(3),0.29727812024999545)\n",
      "(CompactBuffer(e),0.41901815510785123)\n",
      "(CompactBuffer(2),0.1593986567829273)\n",
      "Scores:\n",
      "(3,0.29727812024999545)\n",
      "(0,0.041435022619741926)\n",
      "(2,0.041435022619741926)\n",
      "(3,0.041435022619741926)\n",
      "(2,0.1593986567829273)\n",
      "()\n",
      "ranks after:\n",
      "(0,0.16176112718719904) (1,0.12654135796041838) (2,0.2972499854526872) (3,0.4144475293996951)\n",
      "Iteration:\n",
      "7\n",
      "Iterator(CompactBuffer(3),0.2972499854526872)\n",
      "(CompactBuffer(0, 2, 3),0.12654135796041838)\n",
      "(CompactBuffer(2),0.16176112718719904)\n",
      "(CompactBuffer(e),0.4144475293996951)\n",
      "Scores:\n",
      "(0,0.04218045265347279)\n",
      "(2,0.04218045265347279)\n",
      "(3,0.04218045265347279)\n",
      "(2,0.16176112718719904)\n",
      "(3,0.2972499854526872)\n",
      "()\n",
      "ranks after:\n",
      "(0,0.1614234847528871) (1,0.1255700999974352) (2,0.29892044286200625) (3,0.4140859723876712)\n",
      "Iteration:\n",
      "8\n",
      "Iterator(CompactBuffer(0, 2, 3),0.1255700999974352)\n",
      "(CompactBuffer(2),0.1614234847528871)\n",
      "(CompactBuffer(e),0.4140859723876712)\n",
      "(CompactBuffer(3),0.29892044286200625)\n",
      "Scores:\n",
      "(0,0.04185669999914507)\n",
      "(2,0.04185669999914507)\n",
      "(3,0.04185669999914507)\n",
      "(2,0.1614234847528871)\n",
      "(3,0.29892044286200625)\n",
      "()\n",
      "ranks after:\n",
      "(0,0.16107146413165344) (1,0.12549326913238013) (2,0.29828142617160747) (3,0.4151538405643588)\n",
      "Iteration:\n",
      "9\n",
      "Iterator(CompactBuffer(2),0.16107146413165344)\n",
      "(CompactBuffer(0, 2, 3),0.12549326913238013)\n",
      "(CompactBuffer(e),0.4151538405643588)\n",
      "(CompactBuffer(3),0.29828142617160747)\n",
      "Scores:\n",
      "(2,0.16107146413165344)\n",
      "(3,0.29828142617160747)\n",
      "(0,0.04183108971079338)\n",
      "(2,0.04183108971079338)\n",
      "(3,0.04183108971079338)\n",
      "()\n",
      "ranks after:\n",
      "(0,0.16127661737410062) (1,0.12572019111992624) (2,0.29818736188600603) (3,0.414815829619967)\n",
      "Iteration:\n",
      "10\n",
      "Iterator(CompactBuffer(3),0.29818736188600603)\n",
      "(CompactBuffer(0, 2, 3),0.12572019111992624)\n",
      "(CompactBuffer(2),0.16127661737410062)\n",
      "(CompactBuffer(e),0.414815829619967)\n",
      "Scores:\n",
      "(0,0.04190673037330875)\n",
      "(2,0.04190673037330875)\n",
      "(3,0.04190673037330875)\n",
      "(3,0.29818736188600603)\n",
      "(2,0.16127661737410062)\n",
      "()\n",
      "ranks after:\n",
      "(0,0.16126908461155542) (1,0.125648363794243) (2,0.29835420937954094) (3,0.4147283422146605)\n",
      "Final result\n",
      "3 has rank:  0.4147283422146606 .\n",
      "0 has rank:  0.16126908461155545 .\n",
      "2 has rank:  0.298354209379541 .\n",
      "1 has rank:  0.12564836379424302 .\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "input_path: String = ./SmallGraph1\n",
       "output_path: String = ./answer_task2\n"
      ]
     },
     "execution_count": 319,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val input_path = \"./SmallGraph1\"\n",
    "val output_path = \"./answer_task2\"\n",
    "\n",
    "PageRank.main(Array(input_path, output_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e6e1d11-c7ff-441e-9a56-549b4fc37458",
   "metadata": {},
   "source": [
    "My FUll working : No printout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "id": "c6329726-be24-42c7-a4b3-c5fde9a16405",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined object PageRank\n"
      ]
     },
     "execution_count": 353,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "object PageRank {\n",
    "\n",
    "    // Do not modify\n",
    "    val PageRankIterations = 10\n",
    "\n",
    "    def calculatePageRank(\n",
    "      inputGraphPath: String,\n",
    "      outputPath: String,\n",
    "      iterations: Int,\n",
    "      spark: SparkSession\n",
    "  ): Unit = {\n",
    "    val sc = spark.sparkContext\n",
    "\n",
    "    // TODO - Your code here\n",
    "    // params\n",
    "    val damping = 0.85\n",
    "    // map followee as key\n",
    "    val edgelist = sc.textFile(inputGraphPath)\n",
    "    val followers_list = edgelist.map { // id(0)   // or use row(1)\n",
    "      _.split(\"\\\\s+\")(0)\n",
    "    }\n",
    "    // val n_followers = followers.count()\n",
    "    // obtain all followee\n",
    "    val followees_list = edgelist\n",
    "      .map {\n",
    "        _.split(\"\\\\s+\")(1)\n",
    "      }\n",
    "    // val n_followees = followees_list.count()\n",
    "    // all body\n",
    "    val all_ids = followers_list.union(followees_list).distinct()\n",
    "    // val n_users = 4\n",
    "    val n_users = if (inputGraphPath.startsWith(\"wasb://\")) {\n",
    "        1006458\n",
    "    } else {\n",
    "        all_ids.count() // already have\n",
    "    }\n",
    "    // dangnling ids\n",
    "    var dangling_edges = all_ids.subtract(followers_list).map(id => (id, \"e\"))\n",
    "    // Form adj graph\n",
    "    // graph adj_list (no dangling)\n",
    "    val glinks_all = edgelist.map { row =>\n",
    "      { // graph edges\n",
    "        val edge = row.split(\"\\\\s+\")\n",
    "        (edge(0), edge(1))\n",
    "      }\n",
    "    }.union(dangling_edges).groupByKey().cache()\n",
    "\n",
    "    ////////////////////////////////////////////////////\n",
    "    // Initialization\n",
    "    // full adjacent list\n",
    "    // ranks\n",
    "    var followers_only_rank = all_ids.subtract(followees_list).distinct().map(id => (id, 0.0)).cache()\n",
    "    var ranks = glinks_all.mapValues { v => 1.0 / n_users }\n",
    "    var is_dangling: Boolean = false\n",
    "    if (iterations > 0) {\n",
    "      // only allow if iterations is not 0\n",
    "      for (i <- 1 to iterations) {\n",
    "        var dangling_ranks = sc.doubleAccumulator(\"sumAccumulator\")\n",
    "        val ranking =glinks_all.join(ranks).values // (CompactBuffer(2),0.25) (CompactBuffer(0, 2, 3),0.25) (CompactBuffer(3),0.25) (CompactBuffer(e),0.25)\n",
    "        val scores = ranking.flatMap {\n",
    "          case (followee, rank) => // contrib (url, rank)\n",
    "            //\n",
    "            var is_dangling: Boolean = followee.count { v => v == \"e\" } == 1\n",
    "            if (!is_dangling) {\n",
    "              // this is not a dangling edge\n",
    "              followee.map(id => { // 0, 2, 3... 2, 3... e\n",
    "                (id, rank / followee.size) // number of followee\n",
    "              })\n",
    "            } else {\n",
    "              // summing up the dangling ranks\n",
    "              dangling_ranks.add(rank)\n",
    "              List() // set as list\n",
    "              // dangling_ranks.add(rank)  instead of +=\n",
    "            }\n",
    "        }.union(followers_only_rank).reduceByKey(_+_)\n",
    "        \n",
    "        /// compute updated rank scores here\n",
    "        // scores.count()//\n",
    "        scores.count()\n",
    "        val dangling_sum = dangling_ranks.value\n",
    "        ranks = scores\n",
    "          .mapValues(scores =>\n",
    "            // (1-damping) * dangling_ranks.value / n_users + damping * scores  // vert algo formula\n",
    "            (1 - damping) / n_users + damping * (scores + dangling_sum / n_users)\n",
    "          ).cache() // (0,0.25) (1,0.25) (2,0.25) (3,0.25)\n",
    "      }\n",
    "    }\n",
    "    // normalize\n",
    "    val rank_sum = ranks.map(e => e._2).sum()\n",
    "    // write to file\n",
    "    //ranks.map{tup => s\"${tup._1}\\t${tup._2}\"}.saveAsTextFile(outputPath)\n",
    "    ranks.mapValues(s => s / rank_sum).foreach(tup => println(s\"${tup._1} has rank:  ${tup._2} .\"))\n",
    "  }\n",
    "\n",
    "  /** @param args\n",
    "    *   it should be called with two arguments, the input path, and the output\n",
    "    *   path.\n",
    "    */\n",
    "  def main(args: Array[String]): Unit = {\n",
    "    val spark = SparkUtils.sparkSession()\n",
    "\n",
    "    val inputGraph = args(0)\n",
    "    val pageRankOutputPath = args(1)\n",
    "    val PageRankIterations = 10\n",
    "    calculatePageRank(inputGraph, pageRankOutputPath, PageRankIterations, spark)\n",
    "  }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "id": "d7c6949a-b3d0-48c3-ba4d-b92fb5f75c63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 has rank:  0.12564836379424302 .\n",
      "0 has rank:  0.16126908461155545 .\n",
      "3 has rank:  0.41472834221466054 .\n",
      "2 has rank:  0.298354209379541 .\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "input_path: String = ./SmallGraph1\n",
       "output_path: String = ./answer_task2(2)\n"
      ]
     },
     "execution_count": 354,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val input_path = \"./SmallGraph1\"\n",
    "val output_path = \"./answer_task2(2)\"\n",
    "\n",
    "PageRank.main(Array(input_path, output_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "848de010-38d2-4a90-8d1f-572a222082c3",
   "metadata": {},
   "source": [
    "Reference Page Rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "id": "a69a683a-1496-477b-aa4e-4fc6a944cdc7",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "ename": "<console>",
     "evalue": "69: error: not found: value myRDD",
     "output_type": "error",
     "traceback": [
      "<console>:69: error: not found: value myRDD",
      "                 myRDD.collect().foreach(println)",
      "                 ^",
      ""
     ]
    }
   ],
   "source": [
    "import org.apache.spark.sql.SparkSession\n",
    "\n",
    "object PageRank2 {\n",
    "\n",
    "    // Do not modify\n",
    "    val PageRankIterations = 10\n",
    "\n",
    "    def calculatePageRank(\n",
    "        inputGraphPath: String,\n",
    "        outputPath: String,\n",
    "        iterations: Int,\n",
    "        spark: SparkSession): Unit = {\n",
    "        val sc = spark.sparkContext\n",
    "        val txt = sc.textFile(inputGraphPath).distinct()\n",
    "    \n",
    "    \n",
    "        val iters = iterations\n",
    "        val lines = spark.read.textFile(inputGraphPath).rdd\n",
    "        val links = lines.map{ s =>\n",
    "          val parts = s.split(\"\\\\s+\")\n",
    "          (parts(0), parts(1))\n",
    "        }.distinct().groupByKey().cache()\n",
    "        var ranks = links.mapValues(v => 1.0)\n",
    "        println(\"links & ranks start:\")\n",
    "        println(links.collect().mkString(\" \"))\n",
    "        println(ranks.collect().mkString(\" \"))\n",
    "        for (i <- 1 to iters) {\n",
    "          val contribs = links.join(ranks).values.flatMap{ case (urls, rank) =>\n",
    "            val size = urls.size\n",
    "            urls.map(url => (url, rank / size))\n",
    "          }\n",
    "          ranks = contribs.reduceByKey(_ + _).mapValues(0.15 + 0.85 * _)\n",
    "          println(\"contribs after:\")\n",
    "          println(contribs.collect().mkString(\" \"))\n",
    "          println(\"ranks after:\")\n",
    "          myRDD.collect().foreach(println)\n",
    "        }\n",
    "\n",
    "        val output = ranks.collect()\n",
    "        output.foreach(tup => println(s\"${tup._1} has rank:  ${tup._2} .\"))\n",
    "\n",
    "        spark.stop()\n",
    "              \n",
    "    }\n",
    "    /**\n",
    "    * @param args it should be called with two arguments, the input path, and the output path.\n",
    "    */\n",
    "    def main(args: Array[String]): Unit = {\n",
    "        val spark = SparkUtils.sparkSession()\n",
    "\n",
    "        val inputGraph = args(0)\n",
    "        val pageRankOutputPath = args(1)\n",
    "        val PageRankIterations = 10\n",
    "        calculatePageRank(inputGraph, pageRankOutputPath, PageRankIterations, spark)\n",
    "  }\n",
    "}\n",
    "PageRank2.main(Array(input_path, output_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96487010-bb04-4d3f-bd35-be9a190bbb7c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "// print\n",
    "val rddWhole = spark.sparkContext.wholeTextFiles(\"./answer_task2\")\n",
    "println(rddWhole.getClass)\n",
    "  rddWhole.foreach(f=>{\n",
    "    println(f._1+\"=>\"+f._2)\n",
    "  })"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
